<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>ELBO — What &amp; Why | Yunfan’s Blog</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="ELBO — What &amp; Why" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ELBO (evidence lower bound) is a key concept in Variational Bayesian Methods. It transforms inference problems, which are always intractable, into optimization problems that can be solved with, for example, gradient-based methods." />
<meta property="og:description" content="ELBO (evidence lower bound) is a key concept in Variational Bayesian Methods. It transforms inference problems, which are always intractable, into optimization problems that can be solved with, for example, gradient-based methods." />
<link rel="canonical" href="https://yunfanj.com/blog/2021/01/11/ELBO.html" />
<meta property="og:url" content="https://yunfanj.com/blog/2021/01/11/ELBO.html" />
<meta property="og:site_name" content="Yunfan’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-11T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ELBO — What &amp; Why" />
<script type="application/ld+json">
{"headline":"ELBO — What &amp; Why","dateModified":"2021-01-11T00:00:00+00:00","datePublished":"2021-01-11T00:00:00+00:00","description":"ELBO (evidence lower bound) is a key concept in Variational Bayesian Methods. It transforms inference problems, which are always intractable, into optimization problems that can be solved with, for example, gradient-based methods.","url":"https://yunfanj.com/blog/2021/01/11/ELBO.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://yunfanj.com/blog/2021/01/11/ELBO.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://yunfanj.com/blog/feed.xml" title="Yunfan's Blog" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-207503775-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  </script>
  
  <script type="text/javascript"
     src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js">
  </script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Yunfan&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class='page-link' href='https://yunfanj.com/' target='_blank'>About</a>
            <a class='page-link' href='https://github.com/yunfanjiang/blog' target='_blank'>Site Code</a>
            <!-- <a class="page-link" href="/blog/about/">About</a> --></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">ELBO — What &amp; Why</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-01-11T00:00:00+00:00" itemprop="datePublished">Jan 11, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>ELBO (evidence lower bound) is a key concept in <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" target="_blank">Variational Bayesian Methods</a>. It transforms inference problems, which are always <em>intractable</em>, into optimization problems that can be solved with, for example, gradient-based methods.</p>

<details>
    <summary><h2>Updates</h2></summary>
    <b>&#8226; May 26, 2021</b>
    <p>I rewrote the entire story, added more figures, but left derivations unchanged. Wish that the second version could better help you :).
    </p>
    <b>&#8226; April 16, 2021</b>
    <p>An extensional derivation for the case of temporal sequences has been updated <a href="#temp_seq">here</a>.
    </p>
</details>

<h2 id="introduction">Introduction</h2>

<p>In this post, I’ll introduce an important concept in  <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" target="_blank">Variational Bayesian (VB) Methods</a> — ELBO (evidence lower bound, also known as variational lower bound) — and its derivations, alongside some digging into it. ELBO enables the rewriting of statistical inference problems as optimization problems, the so-called <a href="https://blog.evjang.com/2016/08/variational-bayes.html" target="_blank"><em>inference-optimization duality</em></a>. Combined with optimization methods such as gradient descent and modern approximation techniques, e.g., deep neural networks, inference on complex distributions can be achieved. Numerous applications can be found in <a href="https://arxiv.org/abs/1312.6114v10" target="_blank">VAE</a>, <a href="https://arxiv.org/abs/1806.02426" target="_blank">DVRL</a>, <a href="https://arxiv.org/abs/1803.10760" target="_blank">MERLIN</a>, to name a few.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#motivation">Motivation</a></li>
  <li><a href="#amortized_vi">Amortized Variational Inference and Evidence Lower Bound</a></li>
  <li><a href="#derivation_kl">How Good is the <i>Variational Posterior</i></a></li>
  <li><a href="#temp_seq">Extension: ELBO for Temporal Sequence</a></li>
  <li><a href="#summary">Summaries</a></li>
</ul>

<h2 id="motivation">Motivation</h2>

<p>We are interested in finding the distribution \(p\left(x\right)\) of some given observations \(x\). Sometimes, this distribution can be fairly simple. For example, if the observations are the outcomes of flipping a coin, \(p\left(x\right)\) will be a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution" target="_blank">Bernoulli distribution</a>. In a continuous case, \(p\left(x\right)\) will be a simple <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank">Gaussian distribution</a> if you are measuring the heights of people. However, sadly, we generally encounter observations with complicated distributions. The <a href="#fig1">figure</a> below, for instance, shows such a \(p\left(x\right)\), which is a <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model" target="_blank">mixed Gaussian distribution</a>.</p>

<p class="center"><span id="fig1"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig1.svg" title="The distribution shown is fairly complicated. It is a Gaussian mixture model with 3 Gaussian distributions." /></span></p>

<p>Similar to <a href="https://en.wikipedia.org/wiki/Law_of_total_probability" target="_blank">the law of total probability</a> which relates marginal probabilities to conditional probabilities, we can think that the distribution of interest \(p\left(x\right)\) can be transformed from a simple distribution, let’s say, \(p\left(z\right)\). We will assume that \(p\left(z\right)\) is <a href="#fig2">a simple Gaussian distribution</a>. Any other types of distributions can play the same role.</p>

<p class="center"><span id="fig2"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig2.svg" title="p(z) is a simple Gaussian distribution." /></span></p>

<p>Now we will try to use \(p\left(z\right)\) with some transformation \(f\left(\cdot\right)\) to fit \(p\left(x\right)\). Concretely, we select several shifted copies of \(p\left(z\right)\) and multiply each of them with a weight \(w_i\). The result is shown in the <a href="#fig3">figure</a> below.</p>

<p class="center"><span id="fig3"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig3.svg" title="A demo to fit p(x) with p(z) and some transformation" /></span></p>

<p>I have to say that this is not a bad fitting consider its simplicity. We can improve this fitting by tweaking the weights, which leads to the following <a href="#fig4">fitting</a>.</p>

<p class="center"><span id="fig4"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig4.svg" title="We tweak the weights to improve the fitting." /></span></p>

<p>Let’s define this intuition formally. Given observations \(x\), we can build a <a href="https://en.wikipedia.org/wiki/Latent_variable_model" target="_blank">latent variable model</a> with the variable \(z\) (we call it “<em>latent</em>” as it is not observed) such that the distribution of interest \(p\left(x\right)\) can be decomposed as 
\(\begin{align}
p\left(x\right) = \int_z p\left(x \vert z\right) p\left(z\right) dz.
\tag{1}
\end{align}\)</p>

<p>The intuition behind Eq. (1) is: We condition our observations on some variables that we don’t know. Therefore, the probability of observations will be the multiplication of the conditional probability and the prior probability of those unknown variables. Subsequently, we integrate out all cases of unknowns to get the distribution of interest. In the above naive case, the shift means and those weights we applied correspond to the term \(p\left(x \vert z\right)\), while the transformation, i.e., the summation, corresponds to the integration.</p>

<p>Despite the convenience provided by decomposing a very complicated distribution into the multiplication of a simple Gaussian conditional distribution and a Gaussian prior distribution, there is a PROBLEM — the integration. It is <em>intractable</em> because it is performed over the whole latent space, which is impractical when latent variables are continuous.</p>

<p>Besides the above-mentioned intractable integration, another question would be that how to obtain a function that transforms \(p\left(z\right)\) into \(p\left(x\right)\). In other words, how to get the conditional distribution \(p\left(x \vert z\right)\). Despite that this function seems to be extremely non-linear, it is still not difficult to solve this problem as we know that neural networks are universal function approximators that can approximate any functions to arbitrary precisions. Therefore, we could use a neural network with parameters \(\theta\) to approximate the distribution \(p\left(x \vert z\right)\), which gives us \(p_\theta \left(x \vert z\right)\). In the subsequent sections, we will see how we can avoid the intractable integration and optimize our parameters.</p>

<h2 id="amortized_vi">Amortized Variational Inference and Evidence Lower Bound</h2>

<p>Parameters \(\theta\) can be obtained from <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" target="_blank">maximum likelihood estimation</a>: \(\theta = \arg\max_\theta \log p_\theta \left(x\right)\). To avoid integrating over the whole latent space, a natural question would be “Can we infer any information about \(z\) after observing a sample \(x_i \in \mathcal{X}\)?”. The answer is “Yes” and we can use \(q_i\left(z\right)\) to approximate the distribution of \(z\) given the \(i\)-th observation. This idea is the <a href="https://en.wikipedia.org/wiki/Statistical_inference" target="_blank">statistical inference</a>, which aims to infer the value of one random variable given the observed value of another random variable. Nevertheless, there is an obvious drawback behind this intuition. The number of parameters of \(q_i \left(z\right)\) will scale up with the size of the set of observations because we build individual distribution after observing each data. To alleviate this problem, we introduce another network with parameters \(\phi\) to parameterize an approximation of \(q_i \left(z\right)\), i.e., \(q_\phi \left(z \vert x\right) \approx q_i \left(z\right) \forall x_i \in \mathcal{X}\), such that the increase of the number of parameters is <em>amortized</em>. This is the <em>amortized variational inference</em>, which is also referred to as <em>variational inference</em> in recent literature. Up to this point, as shown <a href="#fig5">below</a>, we have explicitly built a probabilistic graphical model to represent our problem where the observation \(x\) is conditioned on the latent variable \(z\) and we aim to infer \(z\) after observing \(x\).</p>

<p class="center"><span id="fig5"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig5.svg" title="A probabilistic graphical model showing relations between x and z" width="10%" /></span></p>

<p>Now let’s revisit our objective to maximize the log-likelihood of observations \(x\) but with \(q_\phi \left(z \vert x\right)\) this time.</p>

\[\begin{align}
\log p_\theta(x) &amp;= \log \int_z p_\theta(x, z) dz \\
&amp;= \log \int_z p_\theta(x, z) \frac{q_\phi(z \vert x)}{q_\phi(z \vert x)} dz \\
&amp;= \log \mathbb{E}_{z \sim q_\phi(z \vert x)} \left[ \frac{p_\theta(x, z)}{q_\phi(z \vert x)}\right] \\
&amp;\geq \mathbb{E}_z \left[ \log \frac{p_\theta(x,z)}{q_\phi(z \vert x)}\right] \text{by Jensen's inequality} \\
&amp;= \mathbb{E}_z \left[ \log p_\theta(x,z) \right] + \int_z q_\phi(z \vert x) \log \frac{1}{q_\phi(z \vert x)} dz \\
&amp;= \mathbb{E}_z \left[ \log p_\theta(x,z) \right] + \mathcal{H} \left(q_\phi \left(z \vert x\right) \right)
\tag{2}.
\end{align}\]

<p>In the above equation, the term \(\mathcal{H}\left(\cdot\right)\) is the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank">Shannon entropy</a>. By definition, the term “<em>evidence</em>” is the value of a likelihood function evaluated with fixed parameters. With the definition of \(\mathcal{L} = \mathbb{E}_z \left[ \log p_\theta(x,z) \right] + \mathcal{H} \left(q_\phi \left(z \vert x\right) \right)\), it turns out that \(\mathcal{L}\) sets a lower bound for the evidence of observations and maximizes \(\mathcal{L}\) will push up the log-likelihood of \(x\). Hence, we call \(\mathcal{L}\) the <em>evidence lower bound</em> (ELBO, sometimes referred to as <em>variational lower bound</em> as well).</p>

<p>Now let’s think about the rationale behind \(\mathcal{L}\). First, we focus on the term \(\mathbb{E}_z \left[ \log p_\theta(x,z) \right]\) where \(z \sim q_\phi \left(z \vert x\right)\). Assuming that the neural network with parameters \(\theta\) gives us the joint distribution \(p_\theta \left(x, z\right)\), the optimal distribution \(q_\phi^\ast \left(z \vert x\right)\) that maximizes \(\mathcal{L}\) will be a <a href="https://en.wikipedia.org/wiki/Dirac_delta_function" target="_blank">Dirac delta</a> which puts all the probability mass at the maximum of \(p_\theta \left(x ,z\right)\). The interpretation is as follows. The operation of taking expectation is to just take a weighted average. In the case where data being averaged are fixed but weights can be varied (with the constraint that all weights sum to one), you just need to put 1 for the largest data point and 0 for others to maximize that average. With this intuition, we get the optimal distribution \(q_\phi^\ast \left(z \vert x\right)\) <a href="#fig7">shown</a> below.</p>

<p class="center"><span id="fig7"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig7.svg" title="The optimal distribution is a Dirac delta." /></span></p>

<p>However, the story becomes different when we consider the second term in \(\mathcal{L}\), i.e., the entropy term. This term tells us the uncertainty of a distribution. Samples drawn from a distribution with higher entropy will become more uncertain. Sadly, the entropy of the optimal distribution \(q_\phi^\ast \left(z \vert x\right)\) we have just found is negative infinity. We can show this by constructing a random variable \(x\) drawn from a uniform distribution \(x \sim \mathcal{U} \left(x_0 - \epsilon, x_0 + \epsilon\right)\). Its entropy is \(\mathbb{E}_x \left[\log \frac{1}{p\left(x\right)}\right] = \log\left(2 \epsilon\right)\). As \(\epsilon\) approaching zero, this distribution degenerates to a Dirac delta with entropy \(\lim_{\epsilon \to 0}\log\left(2\epsilon\right) = -\infty\). The <a href="#fig8">figure</a> below shows the entropy varies as a function of \(q_\phi \left(z \vert x\right)\).</p>

<p class="center"><span id="fig8"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig8.svg" title="The entropy varies as a function of different distributions." /></span></p>

<p>Put all of them together, the maximization of \(\mathcal{L}\) tries to find an optimal distribution \(q_\phi^\ast \left(z \vert x\right)\) which not only fits peaks of \(p_\theta \left(x,z\right)\) but also spreads as wide as possible. A visualization is given in the <a href="#fig9">demo</a> below.</p>

<p class="center"><span id="fig9"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig9.gif" title="A visualization of maximizing ELBO" /></span></p>

<p>The neural network with parameters \(\phi\) is sometimes called the <em>inference network</em>, with the distribution \(q_\phi\left(z \vert x\right)\) that it parameterizes named as the <em>variational posterior</em>.</p>

<h2 id="derivation_kl">How Good is the <i>Variational Posterior</i></h2>

<p>We care about the accuracy of the approximation performed by the inference network. As we mentioned earlier, the amortized variational inference leverages a distribution \(q_\phi \left(z \vert x\right)\) to approximate the true <em>posterior</em> of \(z\) given \(x\), i.e., \(p\left(z \vert x\right)\). We choose <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank">Kullback–Leibler divergence</a> as the metric to measure how close is \(q_\phi \left(z \vert x\right)\) to \(p(z \vert x)\).</p>

\[\begin{align}
D_{KL}\left(q_\phi(z \vert x) \Vert p(z \vert x)\right) &amp;= \int_z q_\phi (z \vert x) \log \frac{q_\phi(z \vert x)}{p(z \vert x)} dz\\
&amp;= -\int_z q_\phi(z \vert x) \log \frac{p(z \vert x)}{q_\phi(z \vert x)} dz\\
&amp;= -\int_z q_\phi(z \vert x) \log \frac{p(z,x)}{q_\phi(z \vert x)p(x)} dz \\
&amp;= - \left( \int_z q_\phi(z \vert x) \log \frac{p(z,x)}{q_\phi(z \vert x)} dz - \int_z q_\phi(z \vert x) \log p(x) dz\right) \\
&amp;= - \int_z q_\phi(z \vert x) \log \frac{p(z,x)}{q_\phi(z \vert x)} dz + \log p(x).
\tag{3}
\end{align}\]

<p>It is easy to show that the term \(\int_z q_\phi(z \vert x) \log \frac{p(z,x)}{q_\phi(z \vert x)} dz\) is equal to \(\mathcal{L}\), i.e., ELBO we defined previously. Rewriting Eq. (3) gives</p>

\[\begin{align}
\log p \left(x\right) = \mathcal{L} + D_{KL}\left(q_\phi(z \vert x) \Vert p(z \vert x)\right).
\tag{4}
\end{align}\]

<p>Although the true <em>posterior</em> \(p \left(z \vert x\right)\) is unknown and hence we cannot calculate the KL divergence term analytically, an important property of non-negativity of KL divergence allows us to write Eq. (4) into an inequality:
\(\begin{align}
\log p\left(x\right) \geq \mathcal{L},
\tag{5}
\end{align}\)
which is consistent with Eq. (2) we derived before.</p>

<p>Another way to investigate ELBO is to rewrite it in the following way.
\(\begin{align}
\mathcal{L} &amp;=  \int_z q_\phi(z \vert x) \log \frac{p_\theta \left(x, z\right)}{q_\phi(z \vert x)} dz \\
&amp;= \int_z q_\phi(z \vert x) \log \frac{p_\theta \left(x \vert z\right)p\left(z\right)}{q_\phi(z \vert x)} dz \\
&amp;= \mathbb{E}_{z \sim q_\phi \left(z \vert x\right)} \left[p_\theta\left(x \vert z\right)\right] - D_{KL} \left(q_\phi \left(z \vert x\right) \Vert p\left(z\right)\right)
\tag{6}
\end{align}\)</p>

<p>It suggests that the ELBO is a trade-off between the reconstruction accuracy against the complexity of the variational <em>posterior</em>. The KL divergence term can be interpreted as a measure of the additional information required to express the <em>posterior</em> relative to the <em>prior</em>. As it approaches zero, the <em>posterior</em> is fully obtainable from the <em>prior</em>. Another intuition behind Eq. (6) is that we draw latent variables \(z\) from an approximated <em>posterior</em> distribution, which is very close to its <em>prior</em>, and then use them to reconstruct our observations \(x\). As the reconstruction gets better, our approximated <em>posterior</em> will become more accurate as well. From the perspective of auto-encoder, the neural network with parameters \(\phi\) is called <em>encoder</em> because it maps from the observation space to the latent space, while the network with parameters \(\theta\) is called <em>decoder</em> because it maps from the latent to the observation space. Readers who are interested in this convention are referred to <a href="https://arxiv.org/abs/1312.6114" target="_blank">Kingma <em>et al.</em></a>.</p>

<p>Additionally, let’s think about the reason behind the KL divergence we used to derive Eq. (3):</p>

\[\begin{align}
D_{KL}\left(q_\phi(z \vert x) \Vert p(z \vert x)\right) = \int_z q_\phi (z \vert x) \log \frac{q_\phi(z \vert x)}{p(z \vert x)} dz.
\tag{7}
\end{align}\]

<p>It suggests that the variational <em>posterior</em> \(q_\phi(z \vert x)\) is prevented from spanning the whole space relative to the true <em>posterior</em> \(p\left(z \vert x\right)\). Consider the case where the denominator in Eq. (7) is zero, the value of $q_\phi(z \vert x)$ has to be zero as well otherwise the KL divergence goes to infinity. The <a href="#fig10">figure</a> below demonstrates this. Note that the green region in the left figure indicates where \(\frac{q_\phi(z \vert x)}{p(z \vert x)} = 0\), while the red region in the right figure indicates where \(\frac{q_\phi(z \vert x)}{p(z \vert x)} = \infty\). In summary, the <a href="https://blog.evjang.com/2016/08/variational-bayes.html" target="_blank">reverse KL divergence</a> has the effect of zero-forcing as minimizing it leads to $q_\phi (z \vert x)$ being squeezed under $p (z \vert x)$.</p>

<p class="center"><span id="fig10"><img src="/blog/assets/img/posts/2021-01-11-ELBO/fig10.svg" title="The zero-forcing effect of reverse KL divergence." /></span></p>

<h2 id="temp_seq">Extension: ELBO for Temporal Sequence</h2>

<p>Consider the case that we wish to build a generative model \(p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right)\) for sequential data \(\mathbf{x}_{0:t} \equiv \left(x_0, x_1, \ldots, x_t \right)\) with a sequence of latent variable \(\mathbf{z}_{0:t} \equiv \left(z_0, z_1, \ldots, z_t \right)\), we can also derive a corresponding ELBO as a surrogate objective. Optimizing this objective leads to the maximization of the likelihood of the sequential observations.</p>

\[\begin{align}
\log p \left(\mathbf{x}_{0:t} \right) &amp;= \log \int_{\mathbf{z}_{0:t}} p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) d\mathbf{z}_{0:t} \\
&amp;= \log \int_{\mathbf{z}_{0:t}} p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) \frac{q_\phi\left(\mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}  \right)}{q_\phi\left(\mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}  \right)}d\mathbf{z}_{0:t} \\
&amp;= \log \mathbb{E}_{\mathbf{z}_{0:t} \sim q_\phi \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)} \left[ \frac{p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) }{q_\phi \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)} \right] \\
&amp;\geq \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log \frac{p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) }{q_\phi \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)}\right] \text{by Jensen's inequality}
\tag{8}
\end{align}\]

<p>So far, this is similar to what we have derived for the stationary case, i.e., Eq. (2) in the previous <a href="#amortized_vi">section</a>. However, the following derivation will require some factorizations of the joint distribution and the variational posterior. Concretely, we factorize the temporal model \(p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right)\) and the approximation \(q_\theta \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t} \right)\) as</p>

\[p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) = \prod_{\tau = 0}^t p \left(x_\tau \vert z_\tau\right) p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right),
\tag{9}\]

<p>and</p>

\[q_\phi \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t} \right) = \prod_{\tau=0}^t q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right),
\tag{10}\]

<p>respectively.</p>

<p>To understand these factorizations, we can think that at each time step, the observation conditions on the latent variable at that time step, which also conditions on all latent variables before that time step. Expressing this relation recursively leads to Eq. (9). Similarly, the approximated latent variable at each time step conditions on the sequential observations up to that time and the history of latent variables, which is Eq. (10).</p>

<p>With these two factorizations, we can further derive Eq. (8) by plugging Eq. (9) and Eq. (10):</p>

\[\begin{align}
&amp;\mathbb{E}_{\mathbf{z}_{0:t}} \left[\log \frac{p \left(\mathbf{x}_{0:t}, \mathbf{z}_{0:t} \right) }{q_\phi \left( \mathbf{z}_{0:t} \vert \mathbf{x}_{0:t}\right)}\right] \\
&amp;= \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log \frac{\prod_{\tau = 0}^t p \left(x_\tau \vert z_\tau\right) p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)}{\prod_{\tau=0}^t q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)}\right] \\
&amp;= \mathbb{E}_{\mathbf{z}_{0:t}} \left[\sum_{\tau=0}^t \log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)  \right] \\
&amp;= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right].
\end{align}
\tag{11}\]

<p>Now we will use one trick to replace variables. Note that as the variable \(\tau\) starts from 0 to \(t\), those items being taken expectation, i.e., \(\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\) will become invalid for \(\tau&lt; \tau' \leq t\). Therefore, we can write the original expectation term \(\mathbb{E}_{\mathbf{z}_{0:t}} [\cdot]\) as \(\mathbb{E}_{\mathbf{z}_{0:\tau}} [\cdot]\). Furthermore, another trick will allow us to factorize the expectation. Given the expectation taken <em>w.r.t.</em> \(\mathbf{z}_{0:\tau} \sim q_\phi \left(\mathbf{z}_{0:\tau} \vert \mathbf{x}_{0:\tau}\right)\), i.e., \(\mathbb{E}_{\mathbf{z}_{0:\tau} \sim q_\phi \left(\mathbf{z}_{0:\tau} \vert \mathbf{x}_{0:\tau}\right)}[\cdot]\), we can factorize it as \(\mathbb{E}_{z_\tau \sim q_\phi \left(z_\tau \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)} \mathbb{E}_{\mathbf{z}_{0:\tau-1} \sim q_\phi \left(\mathbf{z}_{0:\tau-1}\vert \mathbf{x}_{0:\tau-1}\right)}[\cdot]\). With these tricks at hands, Eq. (11) can be written as</p>

\[\begin{align}
&amp; \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:t}} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right] \\
&amp;= \sum_{\tau=0}^t \mathbb{E}_{z_\tau}\mathbb{E}_{\mathbf{z}_{0:\tau - 1}} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right] \\
&amp;= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right) + \log p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right) - \log q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right) \right] \\
&amp;= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right) - \log \frac{q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)}{p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)} \right] \\
&amp;= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - \mathbb{E}_{z_\tau}\left[\log \frac{q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)}{p \left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)} \right]\right] \\
&amp;= \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - D_{KL}\left(q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\Vert p\left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)  \right)\right].
\end{align}
\tag{12}\]

<p>Put all of them together, we have derived a lower bound for the log-likelihood of temporal sequence. Great!</p>

\[\begin{align}
&amp;\log p \left(\mathbf{x}_{0:t} \right) \geq \\
&amp;\sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - D_{KL}\left(q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\Vert p\left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)  \right)\right]
\end{align}
\tag{13}\]

<p>If we compare the derived ELBO for temporal sequence, i.e., Eq. (13), with the ELBO for the stationary observation, i.e., Eq. (6), we will find that ELBO for sequential observations is computed firstly by calculating the ELBO for a certain time step. Then this result is taken expectation <em>w.r.t.</em> histories of latent variables considering the property of a sequence. Finally, results are summed up along time step. Don’t be scared by the math, it is fairly easy to understand if we start from the stationary case.</p>

\[\log p \left(\mathbf{x}_{0:t} \right) \geq \sum_{\tau=0}^t \mathbb{E}_{\mathbf{z}_{0:\tau - 1}}\left[  \underbrace{\mathbb{E}_{z_\tau} \left[\log p \left(x_\tau \vert z_\tau\right)\right] - D_{KL}\left(q_\phi \left(z_{\tau} \vert \mathbf{z}_{0:\tau-1}, \mathbf{x}_{0:\tau}\right)\Vert p\left(z_\tau \vert \mathbf{z}_{0:\tau -1}\right)  \right)}_{\text{Eq. (6)}} \right]\]

<h2 id="summary">Summaries</h2>

<p>In this post, we begin with the motivation to fit complicated distributions, then notice the intractable integration, subsequently introduce the amortized variational inference and derive the ELBO from several points of view, and finally, dig deeper facts behind the ELBO. An extension of derivation for temporal sequences is also provided. As I mentioned at the very beginning, it plays an important role because it provides a framework in which statistical inference can be transformed into optimization, leading to more and more amazing applications in the deep learning community.</p>

<p>Thanks for your interest and reading!</p>

  </div><a class="u-url" href="/blog/2021/01/11/ELBO.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Yunfan&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Yunfan&#39;s Blog</li><li><a class="u-email" href="mailto:yunfanjyf@gmail.com">yunfanjyf@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/yunfanjiang" target="_blank"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">yunfanjiang</span></a></li><li><a href="https://www.linkedin.com/in/yunfanjiang" target="_blank"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">yunfanjiang</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Math ∪ Philosophy ∪ Coding</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
