<!DOCTYPE html>
<html lang="zh-CN">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="ReinforcementLearning" />
    <meta name="hexo-theme-A4" content="v1.9.6" />
    <link rel="alternate icon" type="image/webp" href="/img/man.jpg">
    <title>Xuext</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--注意：首页既不是post也不是page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--返回顶部css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--目录-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


<meta name="generator" content="Hexo 7.3.0"></head>
    
    
        <style>
            .index-main{
                max-width:  880px;
            }
        </style>

    
    



    

    
    

    
    
    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        
            <div class="left-toc-container">
                <nav id="toc" class="bs-docs-sidebar"></nav>
            </div>
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <img style="
        width: 56px;
        height: auto;" alt="^-^" cache-control="max-age=86400" class="header-img" src="/img/man.jpg" width="10%"></img>
        <div class="header-content">
            <a class="logo" href="/">Xuext</a> 
            <span class="description">Beihang University</span> 
        </div>
        
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">首页</a></li>
            
        
            
                <li><a href="/list/">⛰️文章</a></li>
            
        
            
                <li><a href="/categories/">🌬️分类</a></li>
            
        
            
                <li><a href="/lover/">💗💗</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--说明是文章post页面-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    ReinforcementLearning
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>最近更新：2024-10-19</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>字数总计：1.2k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>阅读估时：5分钟</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        阅读量：<span id="busuanzi_value_page_pv"></span>次
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E7%AC%AC%E4%B8%80%E8%AE%B2"><span class="post-toc-text">第一讲</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#mdp%E5%86%B3%E7%AD%96"><span class="post-toc-text">MDP决策</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="post-toc-text">值函数估计</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#model--free-rl"><span class="post-toc-text">model- free RL</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#monte-carlo-methods"><span class="post-toc-text">Monte-Carlo methods</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#rl%E4%BB%A3%E7%A0%81"><span class="post-toc-text">RL代码</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#q-learning-%E4%BB%A3%E7%A0%81"><span class="post-toc-text">Q-learning 代码</span></a></li></ol></li></ol></li></ol>
            
        
        <div class=".article-gallery"><h1 id="第一讲">第一讲</h1>
<h2 id="mdp决策">MDP决策</h2>
<p><strong>策略：从状态S到动作的映射map</strong></p>
<p>ML ---输出----&gt; model; RL ---输出----&gt; policy <span class="math inline">\(\pi\)</span></p>
<ul>
<li>确定性策略 Deterministic Policy : <span class="math inline">\(a = \pi (s)\)</span></li>
<li>随机策略 Stochastic Policy : $ (a | s) = P(A_t = a | S_t = s)$</li>
</ul>
<p><strong>奖励：定义RL目标的一个标量</strong></p>
<p><span style="color : red">和环境交互的目标：</span><span class="math inline">\(\max \limits_\pi \mathbb{E}_{\pi, env} [R_0 + \gamma R_1 + \gamma^2 R_2 + ...] = \mathbb{E}_{\pi, env} [\sum_{t=0}^{T} \gamma^t \times r(s_t, a_t)]\)</span></p>
<p>MDP只是强化学习交互的一种方式，🤔很多paper往往先把环境MDP化然后再进行强化学习算法</p>
<p>如果当前MDP已经搭建好了，那么该如何使用动态规划？</p>
<blockquote>
<p><strong>随机过程</strong>：一个或多个事件、随机系统或者随机现象随着时间发生演变的过程</p>
<p><span class="math inline">\(\mathbb{P}[S_{t+1  }| S_1, S_2, ..., S_t]\)</span>​</p>
<p>随机过程中有一个非常重要的类：<strong>马尔可夫过程(Markov Process)</strong> 【具有Markov性质的随机过程】</p>
</blockquote>
<ul>
<li>Markov Process</li>
</ul>
<p>定义：状态<span class="math inline">\(S_t\)</span>是Markov的，<strong>当且仅当</strong><span class="math inline">\(\mathbb{P}[S_{t+1  }| S_t ]= \mathbb{P}[S_{t+1  }| S_1, S_2, ..., S_t]\)</span></p>
<p>未来的分布只需要当前状态即可，也就是说“当前State是未来的充分统计量”。如果此时的MP 加上 reward 就变成了 Markov Reward Process (MRP) 。</p>
<p>奖励函数<span class="math inline">\(R(s_t = s) = \mathbb{E}[r_t | s_t = s]\)</span>记为<span class="math inline">\(R_t\)</span>，<span style="color: green">因为此时的环境是完全已知的，所以需要计算走这一步未来能得到多少回报。</span>序列回报为 $G_t = <em>{k = 0}<sup></sup>k R</em>{t+k} = R_t + R_{t+1} + ... + $</p>
<blockquote>
<p>对于<span class="math inline">\(\gamma\)</span>的取值应该为0到1之间，因为序列长度可能是无限的，为了让得到的<span class="math inline">\(G_t\)</span>是一个可比较的数。当然如果序列长度（time step）是有限的，<span class="math inline">\(\gamma\)</span>​也可以取为1</p>
<p>why ! 要用<span class="math inline">\(\gamma^t\)</span>作为衰减因子？</p>
<p>$a_0 + _1 a_1 &gt; b_0 + _1 b_1 r + _1 a_0 + _2 a_1 &gt; r + _1 b_0 + _2 b_1 $</p>
<p>$a_0 - b_0 + _1 (a_1 - b_1) &gt; 0_1(a_0 - b_0) + _2(a_1 - b_1) &gt;0 $</p>
<p>所以<span class="math inline">\(\gamma_2 = \gamma_1^2\)</span></p>
</blockquote>
<p>MDP：<span class="math inline">\(\mathbb{P}[S_{t+1} | S_t , A_t]\)</span>​</p>
<p>🌟五元组：<span class="math inline">\((S, A, \{ P_{sa}\} , \gamma, r)\)</span></p>
<ol type="1">
<li>S is a finite set of states</li>
<li>A is a finite set of actions</li>
<li>P is dynamics model for each action <span class="math inline">\(P(s_{t+1} | s_{t}, a_{t})\)</span></li>
<li>R is a reward function <span class="math inline">\(R(s_t = s, a_t = a) = \mathbb{E}[r_t |s_t = s, a_t = a]\)</span></li>
</ol>
<blockquote>
<p><a href="image-20241005213012449.png" title="image-20241005213012449" class="gallery-item" style="box-shadow: none;"> <img src="image-20241005213012449.png" alt="image-20241005213012449" style="zoom:40%;" /></a></p>
<p><strong>🤔Q和R的区别？</strong></p>
<p>Q值全称为状态-动作价值函数（State-Action Value Function），衡量的是在给定状态下采取某个动作后，根据当前策略，未来可以获得的奖励的预期总和。通过贝尔曼方程递归定义，指导策略更新！！</p>
<p>R代表即时奖励（Instantaneous Reward），执行某个动作后立即获得的反馈值（单次交互的结果，可正可负）。它是环境对agent行为的直接评价，反映了agent行为的好坏。</p>
</blockquote>
<!-- ### 策略提升和策略评估 -->
<!-- ## 值函数估计和无模型控制 -->
<h1 id="值函数估计">值函数估计</h1>
<p>（model-free RL 蒙特卡洛方法 蒙特卡洛价值预测 重要性采样 时序差分学习）</p>
<h2 id="model--free-rl">model- free RL</h2>
<p>没有MDP情况下，没办法做Dynamic programming，不是直接知道State transition 和 reward function的，但是可以拿到一串串交互出的轨迹数据，基于此去做policy learning</p>
<p>首先在<strong>model-base</strong>的MDP中，value function 可以通过动态规划计算获得</p>
<p><span class="math display">\[
V^\pi(s) = \mathbb{E}[r(S_0) + \gamma r(S_1)+ \gamma^2 r(S_2) + ... | S_0 = s, \pi] = r(s) + \gamma \sum_{s&#39; \in S} P_{s\pi (s)}(s&#39;)V^\pi (s&#39;)
\]</span></p>
<blockquote>
<ul>
<li>即时奖励： <span class="math inline">\(r(s)\)</span> 是在状态 <span class="math inline">\(s\)</span> 下根据策略 <span class="math inline">\(π\)</span> 采取行动所获得的即时奖励。</li>
<li>折扣因子： <span class="math inline">\(γ\)</span> 是折扣因子，它决定了未来奖励相对于即时奖励的价值。</li>
<li>状态转移概率 <span class="math inline">\(P_{s\pi (s)}(s&#39;)\)</span> 是在状态 <span class="math inline">\(s\)</span> 下根据策略 <span class="math inline">\(π\)</span> 采取行动转移到状态 <span class="math inline">\(s′\)</span> 的概率。</li>
<li>未来价值：$V^(s') $ 是从状态 <span class="math inline">\(s&#39;\)</span>开始的期望回报</li>
<li>价值函数是<strong>即时奖励</strong>和所有可能的<strong>未来状态</strong>的期望回报的总和</li>
</ul>
</blockquote>
<p>在Model-free RL中，不依赖于<strong>状态转移概率和奖励函数</strong>的显式模型，直接通过与环境的交互来学习策略或价值函数</p>
<ul>
<li>无法直接获得<span class="math inline">\(P_{sa}\)</span> 和 <span class="math inline">\(r\)</span></li>
<li><span class="math inline">\(r\)</span> 往往可以通过任何一个（s，a） 的结果去做观测</li>
</ul>
<h3 id="monte-carlo-methods">Monte-Carlo methods</h3>
<p>使用MC的方法去估计Q表格</p>
<ul>
<li>Trade-off between exploration and exploitation 可以使用 <span class="math inline">\(\epsilon-greedy\)</span> Exploration：Ensuring continual exploration</li>
</ul>
<p>（刚开始时有80%的概率探索，随着迭代这个概率逐渐减少）</p>
<h1 id="rl代码">RL代码</h1>
<h3 id="q-learning-代码">Q-learning 代码</h3>
<p>gym基本用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;MountainCar-v0&#x27;</span> , render_mode=<span class="string">&#x27;human&#x27;</span>)</span><br><span class="line">env.reset()</span><br><span class="line">done = <span class="literal">False</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">    action = <span class="number">2</span>  <span class="comment"># 0： 往左推， 1：不动， 2：往右推</span></span><br><span class="line">    new_state, reward, done, extra_bool,  _ = env.step(action) <span class="comment"># _ 是预留指</span></span><br><span class="line">    <span class="built_in">print</span>(new_state) <span class="comment"># 环境输出是连续的</span></span><br><span class="line">    env.render()</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<p>Q-learning代码部分选用的是gym的‘MountainCar-v0’环境，通过左右的推动让小车冲上坡顶</p>
<p><a href="image-20241005235331198.png" title="image-20241005235331198" class="gallery-item" style="box-shadow: none;"> <img src="image-20241005235331198.png" alt="image-20241005235331198" style="zoom:20%;" /></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(env.observation_space.high)  <span class="comment"># [0.6,  0.07]</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space.low)   <span class="comment"># [-1.2, -0.07]</span></span><br><span class="line"><span class="built_in">print</span>(env.action_space.n)          <span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<p><strong>x的取值为（-1.2, 0.6） v的取值为（-0.07, 0.07）</strong></p>
</div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2024-10-06</span>
            
                <span>该篇文章被 Xue xt</span>
            
            
             
                <span>归为分类:
                    
                    
                        <a href='/categories/RL/'>
                            RL
                        </a>
                    
                </span>
            
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>上一篇：<a href='/2024/10/06/Transform/'>Transform</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">下一篇：<a href="/2024/10/05/paperRding1005/">paperRding1005</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            © 2001-2024 Xuext 

            
                

            
        </span>
       
    
</div>



<!--这是指一条线往下的内容-->
<div class="footer-last">
    
            <span>🌊看过大海的人不会忘记海的广阔🌊</span>
            
                <span class="footer-last-span-right"><i>本站由<b style="color: #333333;">余金悦女士</b>亲情支持 👏 </i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--目录-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--回到顶部按钮-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // 启用插件
            thumbnail: true,          // 显示缩略图
            zoom: true,               // 启用缩放功
            rotate: true,             // 启用旋转功能能
            autoplay: true,        // 启用自动播放功能
            fullScreen: true,      // 启用全屏功能
            pager: false, //页码,
            zoomFromOrigin: true,   // 从原始位置缩放
            actualSize: true,       // 启用查看实际大小的功能
            enableZoomAfter: 300,    // 延迟缩放，确保图片加载完成后可缩放
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // 修复选择器
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- 回到顶部的按钮-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- 返回的按钮-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>