<!DOCTYPE html>
<html lang="zh-CN">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="Transform" />
    <meta name="hexo-theme-A4" content="v1.9.6" />
    <link rel="alternate icon" type="image/webp" href="/img/man.jpg">
    <title>Xuext</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--æ³¨æ„ï¼šé¦–é¡µæ—¢ä¸æ˜¯postä¹Ÿä¸æ˜¯page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--è¿”å›é¡¶éƒ¨css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--ç›®å½•-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


<meta name="generator" content="Hexo 7.3.0"></head>
    
    
        <style>
            .index-main{
                max-width:  880px;
            }
        </style>

    
    



    

    
    

    
    
    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        
            <div class="left-toc-container">
                <nav id="toc" class="bs-docs-sidebar"></nav>
            </div>
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <img style="
        width: 56px;
        height: auto;" alt="^-^" cache-control="max-age=86400" class="header-img" src="/img/man.jpg" width="10%"></img>
        <div class="header-content">
            <a class="logo" href="/">Xuext</a> 
            <span class="description">Beihang University</span> 
        </div>
        
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">é¦–é¡µ</a></li>
            
        
            
                <li><a href="/list/">â›°ï¸æ–‡ç« </a></li>
            
        
            
                <li><a href="/categories/">ğŸŒ¬ï¸åˆ†ç±»</a></li>
            
        
            
                <li><a href="/lover/">ğŸ’—ğŸ’—</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--è¯´æ˜æ˜¯æ–‡ç« posté¡µé¢-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    Transform
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>æœ€è¿‘æ›´æ–°ï¼š2024-10-21</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>å­—æ•°æ€»è®¡ï¼š3.1k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>é˜…è¯»ä¼°æ—¶ï¼š14åˆ†é’Ÿ</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        é˜…è¯»é‡ï¼š<span id="busuanzi_value_page_pv"></span>æ¬¡
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#transform"><span class="post-toc-text">Transform</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="post-toc-text">è¿ç§»å­¦ä¹ </span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#transform-each-layer"><span class="post-toc-text">Transform each layer</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82"><span class="post-toc-text">æ³¨æ„åŠ›å±‚</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86"><span class="post-toc-text">ä»£ç éƒ¨åˆ†</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#embedding"><span class="post-toc-text">Embedding</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#positional-embedding"><span class="post-toc-text">Positional Embedding</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#wordembedding-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="post-toc-text">WordEmbedding æ·±åº¦è§£æ</span></a></li></ol></li></ol>
            
        
        <div class=".article-gallery"><h1 id="transform">Transform</h1>
<h2 id="è¿ç§»å­¦ä¹ ">è¿ç§»å­¦ä¹ </h2>
<p><a href="transformers_chrono.svg" title="transformers_chrono" class="gallery-item" style="box-shadow: none;"> <img src="transformers_chrono.svg" alt="transformers_chrono" style="zoom:40%;" /></a></p>
<p>æŒ‰æ¨¡å‹ç»“æ„å°†å®ƒä»¬å¤§è‡´åˆ†ä¸ºä¸‰ç±»ï¼š</p>
<ul>
<li><strong>çº¯ Encoder æ¨¡å‹</strong>ï¼ˆä¾‹å¦‚ BERTï¼‰ï¼Œåˆç§°è‡ªç¼–ç  (auto-encoding) Transformer æ¨¡å‹ï¼›é€‚ç”¨äºåªéœ€è¦ç†è§£è¾“å…¥è¯­ä¹‰çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼›</li>
<li><strong>çº¯ Decoder æ¨¡å‹</strong>ï¼ˆä¾‹å¦‚ GPTï¼‰ï¼Œåˆç§°è‡ªå›å½’ (auto-regressive) Transformer æ¨¡å‹ï¼›é€‚ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆï¼›</li>
<li><strong>Encoder-Decoder æ¨¡å‹</strong>ï¼ˆä¾‹å¦‚ BARTã€T5ï¼‰ï¼Œåˆç§° Seq2Seq (sequence-to-sequence) Transformer æ¨¡å‹ã€‚é€‚ç”¨äºéœ€è¦åŸºäºè¾“å…¥çš„ç”Ÿæˆå¼ä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘ã€æ‘˜è¦ã€‚</li>
</ul>
<p><a href="main_transformer_architectures.png" title="main_transformer_architectures" class="gallery-item" style="box-shadow: none;"> <img src="main_transformer_architectures.png" alt="main_transformer_architectures" style="zoom:50%;" /></a></p>
<p>ä¸¤ä¸ªå¸¸ç”¨çš„é¢„è®­ç»ƒä»»åŠ¡ï¼š</p>
<ul>
<li><p>åŸºäºå¥å­çš„å‰ n ä¸ªè¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œå› ä¸ºè¾“å‡ºä¾èµ–äºè¿‡å»å’Œå½“å‰çš„è¾“å…¥ï¼Œå› æ­¤è¯¥ä»»åŠ¡è¢«ç§°ä¸º<strong>å› æœè¯­è¨€å»ºæ¨¡</strong> (causal language modeling)</p></li>
<li><p>åŸºäºä¸Šä¸‹æ–‡ï¼ˆå‘¨å›´çš„è¯è¯­ï¼‰æ¥é¢„æµ‹å¥å­ä¸­è¢«é®ç›–æ‰çš„è¯è¯­ (masked word)ï¼Œå› æ­¤è¯¥ä»»åŠ¡è¢«ç§°ä¸º<strong>é®ç›–è¯­è¨€å»ºæ¨¡</strong> (masked language modeling)</p></li>
</ul>
<p>ä½†æ˜¯å¦‚æœç›´æ¥æ‹¿æ¥å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œæ•ˆæœå¾€å¾€å¹¶ä¸å¥½ã€‚å› æ­¤ï¼Œé€šå¸¸é‡‡ç”¨<strong>è¿ç§»å­¦ä¹  (transfer learning)</strong> æ–¹æ³•ï¼Œä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨è¯­æ–™ï¼Œä»¥<strong><span style="color : red">æœ‰ç›‘ç£å­¦ä¹ </span></strong>çš„æ–¹å¼å¯¹é¢„è®­ç»ƒæ¨¡å‹å‚æ•°è¿›è¡Œ<strong><span style="color : red">å¾®è°ƒ (fine-tune)</span></strong>ï¼Œä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p>é¢„è®­ç»ƒæ˜¯ä¸€ç§ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„æ–¹å¼ï¼šæ‰€æœ‰çš„æ¨¡å‹æƒé‡éƒ½è¢«<strong>éšæœºåˆå§‹åŒ–</strong>ï¼Œç„¶ååœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å¼€å§‹è®­ç»ƒã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ä»…éœ€è¦æµ·é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”æ—¶é—´å’Œç»æµæˆæœ¬éƒ½éå¸¸é«˜ã€‚</p>
<p>å› æ­¤ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œæ˜¯å°†<code>åˆ«äººé¢„è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡</code>é€šè¿‡è¿ç§»å­¦ä¹ åº”ç”¨åˆ° --ã€‹ è‡ªå·±çš„æ¨¡å‹ä¸­ï¼ˆäºŒæ¬¡è®­ç»ƒï¼Œå¾®è°ƒå‚æ•° Fine-tuned language modelï¼‰</p>
<h2 id="transform-each-layer">Transform each layer</h2>
<h3 id="æ³¨æ„åŠ›å±‚">æ³¨æ„åŠ›å±‚</h3>
<p>æ³¨æ„åŠ›å±‚çš„ä½œç”¨å°±æ˜¯è®©æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æ—¶ï¼Œå°†æ³¨æ„åŠ›åªæ”¾åœ¨æŸäº›è¯è¯­ä¸Š</p>
<p>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒEncoder æ¥å—<code>æºè¯­è¨€</code>çš„å¥å­ä½œä¸ºè¾“å…¥ï¼Œè€Œ Decoder åˆ™æ¥å—<code>ç›®æ ‡è¯­è¨€</code>çš„ç¿»è¯‘ä½œä¸ºè¾“å…¥</p>
<ul>
<li><p>Encoderä¸­ï¼Œç”±äºç¿»è¯‘ä¸€ä¸ªè¯è¯­éœ€è¦ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå› æ­¤æ³¨æ„åŠ›å±‚å¯ä»¥è®¿é—®å¥å­ä¸­çš„æ‰€æœ‰è¯è¯­</p></li>
<li><p>Decoderä¸­ï¼Œæ˜¯é¡ºåºåœ°è¿›è¡Œè§£ç ï¼Œåœ¨ç”Ÿæˆæ¯ä¸ªè¯è¯­æ—¶ï¼Œæ³¨æ„åŠ›å±‚åªèƒ½è®¿é—®å‰é¢å·²ç»ç”Ÿæˆçš„å•è¯</p></li>
</ul>
<p><a href="image-20241008215838149.png" title="image-20241008215838149" class="gallery-item" style="box-shadow: none;"> <img src="image-20241008215838149.png" alt="image-20241008215838149" style="zoom:20%;" /></a></p>
<p>é¦–å…ˆå¯¹å¥å­è¿›è¡Œåˆ†è¯ï¼Œç„¶åå°†æ¯ä¸ªè¯è¯­ (token) éƒ½è½¬åŒ–ä¸ºå¯¹åº”çš„è¯å‘é‡ (token embeddings)ï¼Œè¿™æ ·æ–‡æœ¬å°±è½¬æ¢ä¸ºä¸€ä¸ªç”±è¯è¯­å‘é‡ç»„æˆçš„çŸ©é˜µ<span class="math inline">\(\boldsymbol{X} = (x_1, x_2, ..., x_n)\)</span>ï¼Œå…¶ä¸­å…¶ä¸­$ x_i $å°±è¡¨ç¤ºç¬¬ <span class="math inline">\(i\)</span> ä¸ªè¯è¯­çš„è¯å‘é‡ï¼Œç»´åº¦ä¸º <span class="math inline">\(d\)</span>ï¼Œæ•…<span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{nÃ—d}\)</span>ã€‚</p>
<p>æ¯ä¸€ä¸ªè¯è¯­ <span class="math inline">\(x_t\)</span> å¯¹åº”çš„ç¼–ç ç»“æœ<span class="math inline">\(y_t\)</span> :</p>
<ul>
<li><p><strong>RNN</strong>ï¼šé€šè¿‡<code>é€’å½’</code>è®¡ç®—ï¼š<span class="math inline">\(y_t =f(y_{t-1}, x_t)\)</span>ï¼Œé€’å½’å°±å¯¼è‡´äº†ä»–æ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œéœ€è¦é€æ­¥é€’å½’æ‰èƒ½è·å¾—å…¨å±€ä¿¡æ¯ï¼Œå› æ­¤ä¸€èˆ¬ä½¿ç”¨çš„æ˜¯åŒå‘RNN</p></li>
<li><p><strong>CNN</strong>ï¼šé€šè¿‡<code>æ»‘åŠ¨çª—å£</code>åŸºäºå±€éƒ¨ä¸Šä¸‹æ–‡æ¥ç¼–ç æ–‡æœ¬ï¼Œä¾‹å¦‚æ ¸å°ºå¯¸ä¸º 3 çš„å·ç§¯æ“ä½œå°±æ˜¯ä½¿ç”¨æ¯ä¸€ä¸ªè¯è‡ªèº«ä»¥åŠå‰ä¸€ä¸ªå’Œåä¸€ä¸ªè¯æ¥ç”ŸæˆåµŒå…¥å¼è¡¨ç¤ºï¼Œ<span class="math inline">\(y_t =f(x_{t-1}, x_t, x_{t+1})\)</span>ï¼Œå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œä½†æ˜¯æ›´ä¾§é‡äºå±€éƒ¨ä¿¡æ¯ï¼Œéœ€è¦å±‚å å¢å¤§æ„Ÿå—é‡</p></li>
<li><p><strong>Transform</strong>ï¼š<span class="math inline">\(y_t = f(x_t, A, B)\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(A,B\)</span> æ˜¯å¦å¤–çš„è¯è¯­åºåˆ—ï¼ˆçŸ©é˜µï¼‰ï¼Œå¦‚æœå–<span class="math inline">\(A=B=X\)</span> å°±ç§°ä¸º <code>Self-Attention</code>ï¼Œå³ç›´æ¥å°† <span class="math inline">\(x_t\)</span> ä¸è‡ªèº«åºåˆ—ä¸­çš„æ¯ä¸ªè¯è¯­è¿›è¡Œæ¯”è¾ƒï¼Œæœ€åç®—å‡º <span class="math inline">\(y_t\)</span>ã€‚</p></li>
</ul>
<p>è™½ç„¶ Attention æœ‰è®¸å¤šç§å®ç°æ–¹å¼ï¼Œä½†æ˜¯æœ€å¸¸è§çš„è¿˜æ˜¯ Scaled Dot-product Attentionã€‚</p>
<blockquote>
<p><a href="attention.png" title="img" class="gallery-item" style="box-shadow: none;"> <img src="attention.png" alt="img" style="zoom:50%;" /></a></p>
<p>ä¸»è¦æ­¥éª¤ï¼š</p>
<ul>
<li><strong>è®¡ç®—æ³¨æ„åŠ›æƒé‡</strong>ï¼šä½¿ç”¨æŸç§ç›¸ä¼¼åº¦å‡½æ•°åº¦é‡æ¯ä¸€ä¸ª query å‘é‡å’Œæ‰€æœ‰ key å‘é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦ã€‚å¯¹äºé•¿åº¦ä¸º m çš„ Query åºåˆ—å’Œé•¿åº¦ä¸º n çš„ Key åºåˆ—ï¼Œè¯¥æ­¥éª¤ä¼šç”Ÿæˆä¸€ä¸ªå°ºå¯¸ä¸º mÃ—n çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µã€‚
<ul>
<li>Scaled Dot-product Attention ä½¿ç”¨<strong>ç‚¹ç§¯</strong>ä½œä¸ºç›¸ä¼¼åº¦å‡½æ•°ï¼Œè¿™æ ·ç›¸ä¼¼çš„ queries å’Œ keys ä¼šå…·æœ‰è¾ƒå¤§çš„ç‚¹ç§¯ã€‚<u>ç”±äºç‚¹ç§¯å¯ä»¥äº§ç”Ÿä»»æ„å¤§çš„æ•°å­—ï¼Œè¿™ä¼šç ´åè®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§</u>ã€‚å› æ­¤æ³¨æ„åŠ›åˆ†æ•°è¿˜éœ€è¦ä¹˜ä»¥ä¸€ä¸ª<em>ç¼©æ”¾å› å­æ¥æ ‡å‡†åŒ–å®ƒä»¬çš„æ–¹å·®</em>ï¼Œç„¶åç”¨ä¸€ä¸ª softmax æ ‡å‡†åŒ–ã€‚è¿™æ ·å°±å¾—åˆ°äº†æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ <span class="math inline">\(w_{ij}\)</span>ï¼Œè¡¨ç¤ºç¬¬$ i$ ä¸ª query å‘é‡ä¸ç¬¬ <span class="math inline">\(j\)</span>â€‹ ä¸ª key å‘é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦ã€‚</li>
</ul></li>
<li><strong>æ›´æ–° token embeddings</strong>ï¼šå°†æƒé‡ <span class="math inline">\(w_{ij}\)</span> ä¸å¯¹åº”çš„ value å‘é‡ <span class="math inline">\(v_1,â€¦,v_n\)</span>ç›¸ä¹˜ä»¥è·å¾—ç¬¬ <span class="math inline">\(i\)</span> ä¸ª query å‘é‡æ›´æ–°åçš„è¯­ä¹‰è¡¨ç¤º <span class="math inline">\(x_i&#39;=âˆ‘_j w_{ij} v_j\)</span>â€‹ã€‚</li>
</ul>
</blockquote>
<p><span class="math display">\[
\text{Attention} (Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V
\]</span></p>
<p>å…¶ä¸­ <span class="math inline">\(Q \in \mathbb{R}^{mÃ—d_k},K \in \mathbb{R}^{nÃ—d_k},V \in \mathbb{R}^{nÃ—d_v}\)</span>, å…¶å®å°±æ˜¯<span style="color :green "> <strong><span class="math inline">\(m \times d_k,d_k \times n, n \times d_v\)</span></strong> </span> çŸ©é˜µç›¸ä¹˜ =&gt; <span class="math inline">\(m \times d_v\)</span> <span class="math display">\[
\text{Attention} (q_t, K, V) = \sum \limits_{s=1}^n \frac{1}{Z} exp(\frac{&lt;q_r k_s&gt;}{\sqrt{d_k}}) v_s
\]</span> å…¶ä¸­ <span class="math inline">\(Z\)</span> æ˜¯å½’ä¸€åŒ–å› å­ï¼Œ<span class="math inline">\(K,V\)</span> æ˜¯ä¸€ä¸€å¯¹åº”çš„ key å’Œ value å‘é‡åºåˆ—ï¼ŒScaled Dot-product Attention å°±æ˜¯<span style="color : red">é€šè¿‡ <span class="math inline">\(q_t\)</span> è¿™ä¸ª query ä¸å„ä¸ª <span class="math inline">\(k_s\)</span> å†…ç§¯ï¼Œå¹¶ softmax çš„æ–¹å¼æ¥å¾—åˆ° <span class="math inline">\(q_t\)</span> ä¸å„ä¸ª <span class="math inline">\(v_s\)</span> çš„ç›¸ä¼¼åº¦</span>ï¼Œç„¶ååŠ æƒæ±‚å’Œï¼Œå¾—åˆ°ä¸€ä¸ª <span class="math inline">\(d_v\)</span> ç»´çš„å‘é‡ã€‚å…¶ä¸­å› å­$ $ èµ·åˆ°è°ƒèŠ‚ä½œç”¨ï¼Œä½¿å¾—å†…ç§¯ä¸è‡³äºå¤ªå¤§ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">query, key, value, query_mask=<span class="literal">None</span>, key_mask=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">    dim_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.bmm(query, key.transpose(<span class="number">1</span>, <span class="number">2</span>)) / sqrt(dim_k)</span><br><span class="line">    <span class="keyword">if</span> query_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> key_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mask = torch.bmm(query_mask.unsqueeze(-<span class="number">1</span>), key_mask.unsqueeze(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>))</span><br><span class="line">    weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.bmm(weights, value)</span><br></pre></td></tr></table></figure>
<p>æ³¨æ„ï¼ä¸Šé¢çš„åšæ³•ä¼šå¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼šå½“ Q å’Œ K åºåˆ—ç›¸åŒæ—¶ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¼šä¸ºä¸Šä¸‹æ–‡ä¸­çš„ç›¸åŒå•è¯åˆ†é…éå¸¸å¤§çš„åˆ†æ•°ï¼ˆç‚¹ç§¯ä¸º 1ï¼‰ï¼Œè€Œåœ¨å®è·µä¸­ï¼Œç›¸å…³è¯å¾€å¾€æ¯”ç›¸åŒè¯æ›´é‡è¦ã€‚ä¾‹å¦‚å¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œåªæœ‰å…³æ³¨â€œtimeâ€å’Œâ€œarrowâ€æ‰èƒ½å¤Ÿç¡®è®¤â€œfliesâ€çš„å«ä¹‰ã€‚</p>
<p>å› æ­¤ï¼Œå¤šå¤´æ³¨æ„åŠ› (Multi-head Attention) å‡ºç°äº†ï¼</p>
<h2 id="ä»£ç éƒ¨åˆ†">ä»£ç éƒ¨åˆ†</h2>
<h3 id="embedding">Embedding</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tokenEmbedding</span>(nn.Embedding):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(tokenEmbedding, <span class="variable language_">self</span>).__init__(vocab_size, embedding_dim, padding_idx=<span class="number">1</span>) </span><br><span class="line">        <span class="comment">#è¿™é‡Œæ˜¯åœ¨é•¿åº¦ä¸ä¸€çš„æ—¶å€™ï¼Œç”¨1å»å¡«å……</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># ç”¨æ—¶</span></span><br><span class="line">vocab_size = <span class="number">1000</span>    <span class="comment"># æ˜¯æ€»çš„è¯æ±‡è¡¨çš„é•¿åº¦ï¼Œä¸æ˜¯è¾“å…¥tokençš„é•¿åº¦</span></span><br><span class="line">embedding_dim = <span class="number">200</span>  <span class="comment"># åµŒå…¥vertor dim</span></span><br><span class="line"></span><br><span class="line">embedding_layer = tokenEmbedding(vocab_size, embedding_dim)</span><br><span class="line">input_indices = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype = torch.long)</span><br><span class="line"></span><br><span class="line">embedded_vectors = embedding_layer(input_indices)</span><br><span class="line"><span class="built_in">print</span>(embedded_vectors)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>embedding_layeræœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ï¼Œæ—¢<strong>åµŒå…¥çŸ©é˜µweight</strong>ï¼Œshape=(vocab_size, embedding_dim)ã€‚æ¯ä¸€è¡Œä»£è¡¨è¯æ±‡è¡¨ä¸­ä¸€ä¸ªå…ƒç´ çš„å‘é‡è¡¨ç¤ºï¼ˆæ‰€ä»¥ç›´æ¥æŸ¥è¡¨å°±èƒ½å¾—åˆ° <span class="math inline">\((n,d_{embed})\)</span> çš„çŸ©é˜µï¼‰ã€‚</p>
<h3 id="positional-embedding">Positional Embedding</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_embed, max_len, device</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param d_embed: input_embeddingçš„ç»´åº¦</span></span><br><span class="line"><span class="string">        :param max_len: è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦</span></span><br><span class="line"><span class="string">        :param device: hardware device setting</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦</span></span><br><span class="line">        <span class="variable language_">self</span>.encoding = torch.zeros(max_len, d_embed, device=device)</span><br><span class="line">        <span class="variable language_">self</span>.encoding.requires_grad = <span class="literal">False</span>  <span class="comment"># we don&#x27;t need to compute gradient</span></span><br><span class="line"></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, device=device)</span><br><span class="line">        pos = pos.<span class="built_in">float</span>().unsqueeze(dim=<span class="number">1</span>)   <span class="comment"># 1D =&gt; 2D pos = [[0],[1],[2],...,[max_len]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        _2i = torch.arange(<span class="number">0</span>, d_embed, step=<span class="number">2</span>, device=device).<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment"># &#x27;i&#x27; means index of d_embed(e.g. embedding size = 50, &#x27;i&#x27; = [0,50])</span></span><br><span class="line">        <span class="comment"># &quot;step=2&quot; means &#x27;i&#x27; multiplied with two (same with 2 * i)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># å¾—åˆ°[max_len, d_embed/2]å¤§å°çš„çŸ©é˜µ, åˆ©ç”¨äº†pythonçš„é€å…ƒç´ è¿ç®—</span></span><br><span class="line">        <span class="variable language_">self</span>.encoding[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos / (<span class="number">10000</span> ** (_2i / d_embed)))</span><br><span class="line">        <span class="variable language_">self</span>.encoding[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos / (<span class="number">10000</span> ** (_2i / d_embed)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># self.encoding [max_len = 512, d_embed= 512]</span></span><br><span class="line">        batch_size, seq_len = x.size()</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoding[:seq_len, :] <span class="comment">#è¿”å›å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç </span></span><br></pre></td></tr></table></figure>
<p>Self.encodingæ˜¯ä¸€ä¸ªçŸ©é˜µç±»ä¼¼äºï¼š</p>
<p><a href="image-20241006224518020.png" title="image-20241006224518020" class="gallery-item" style="box-shadow: none;"> <img src="image-20241006224518020.png" alt="image-20241006224518020" style="zoom:50%;" /></a></p>
<p>æœ‰ä¸Šé¢ä¸¤ä¸ªå°±å¯ä»¥å¾—åˆ°transform embeddingä¸ºï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    token embedding + positional encoding (sinusoid)</span></span><br><span class="line"><span class="string">    positional encoding can give positional information to network</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, d_embed, max_len, drop_prob, device</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param vocab_size: size of vocabulary</span></span><br><span class="line"><span class="string">        :param d_model: dimensions of model</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEmbedding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.tok_emb = TokenEmbedding(vocab_size, d_embed)</span><br><span class="line">        <span class="variable language_">self</span>.pos_emb = PositionalEncoding(d_embed, max_len, device)</span><br><span class="line">        <span class="variable language_">self</span>.drop_out = nn.Dropout(p=drop_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        tok_emb = <span class="variable language_">self</span>.tok_emb(x).to(<span class="variable language_">self</span>.device)</span><br><span class="line">        pos_emb = <span class="variable language_">self</span>.pos_emb(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.drop_out(tok_emb + pos_emb)</span><br></pre></td></tr></table></figure>
<h2 id="wordembedding-æ·±åº¦è§£æ">WordEmbedding æ·±åº¦è§£æ</h2>
<p>è¿™éƒ¨åˆ†WordEmbeddingçš„ä»£ç å®è·µï¼é¦–å…ˆå¯¼å…¥ç›¸å…³åº“ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> torch.distributions.uniform <span class="keyword">import</span> Uniform</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lightning <span class="keyword">as</span> L</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>
<p>ä¸»è¦æ€æƒ³å°±æ˜¯ï¼Œé¦–å…ˆéœ€è¦å¯¹è¾“å…¥çš„å¥å­ï¼šâ€œTroll2 is greatâ€å’Œâ€œGymkata is greatâ€ tokenåŒ–ï¼Œä¹Ÿå°±æ˜¯å…ˆåšä¸ªone-hotç¼–ç ã€‚æ€»å…±å››ä¸ªä¸ä¸€æ ·çš„å•è¯ï¼Œæ‰€ä»¥onehotç¼–ç ä¸º4*4æ–¹æ ¼ã€‚<strong>Troll2</strong>çš„ç¼–ç ä¸º[1,0,0,0]ï¼Œç»è¿‡ç¥ç»ç½‘ç»œä¹‹åï¼Œç†æƒ³çš„è¾“å‡ºæ˜¯[0,1,0,0]ï¼Œä¹Ÿå°±æ˜¯è¾“å‡º<strong>is</strong>çš„onehotç¼–ç ã€‚æ‰€ä»¥ç”±æ­¤å¯ä»¥æ„é€ å‡º<strong><u>inputå’Œoutput</u></strong>ã€‚æ„é€ å®Œä¹‹åè®°å¾—ä½¿ç”¨TensorDatasetå’ŒDataLoaderè½¬ä¸ºLoaderæ ¼å¼ï¼Œæ–¹ä¾¿åé¢çš„batchè®­ç»ƒã€‚</p>
<p><a href="wordEmbedding.png" title="image-20241006211311257" class="gallery-item" style="box-shadow: none;"> <img src="wordEmbedding.png" alt="image-20241006211311257" style="zoom:30%;" /></a></p>
<p>ä¸‹é¢æ˜¯å¤æ‚ç‰ˆæœ¬çš„ä»£ç ï¼Œæ²¡æœ‰ä½¿ç”¨nn.Linear()ç®€åŒ–çš„ä»£ç ï¼Œç›¸å¯¹æ¥è¯´æ¯”è¾ƒæ‚ä¹±ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">intputs = torch.tensor([[<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>],</span><br><span class="line">                       [<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>],</span><br><span class="line">                       [<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">0.</span>],</span><br><span class="line">                       [<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">labels = torch.tensor([[<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>],</span><br><span class="line">                       [<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">0.</span>],</span><br><span class="line">                       [<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">0.</span>,<span class="number">1.</span>],</span><br><span class="line">                       [<span class="number">0.</span>,<span class="number">1.</span>,<span class="number">0.</span>,<span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line">dataset = TensorDataset(intputs, labels)</span><br><span class="line">dataLoader = DataLoader(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WordEmbeddingFromScratch</span>(L.LightningModule):</span><br><span class="line">    <span class="comment"># create and initialize weight tensors and create the loss function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        min_value = -<span class="number">0.5</span></span><br><span class="line">        max_value = <span class="number">0.5</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.input1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.input4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.output1_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output1_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output2_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output2_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output3_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output3_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output4_w1 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line">        <span class="variable language_">self</span>.output4_w2 = nn.Parameter(Uniform(min_value, max_value).sample())</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># origin input will be embedding a list ==&gt; [[1,0,0,0]]</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>[<span class="number">0</span>]      <span class="comment"># erase the outside list</span></span><br><span class="line"></span><br><span class="line">        input_to_top_hidden = ((<span class="built_in">input</span>[<span class="number">0</span>] * <span class="variable language_">self</span>.input1_w1) + </span><br><span class="line">                               (<span class="built_in">input</span>[<span class="number">1</span>] * <span class="variable language_">self</span>.input2_w1) + </span><br><span class="line">                               (<span class="built_in">input</span>[<span class="number">2</span>] * <span class="variable language_">self</span>.input3_w1) + </span><br><span class="line">                               (<span class="built_in">input</span>[<span class="number">3</span>] * <span class="variable language_">self</span>.input4_w1))</span><br><span class="line">        input_to_bottom_hidden = ((<span class="built_in">input</span>[<span class="number">0</span>] * <span class="variable language_">self</span>.input1_w2) + </span><br><span class="line">                               (<span class="built_in">input</span>[<span class="number">1</span>] * <span class="variable language_">self</span>.input2_w2) + </span><br><span class="line">                               (<span class="built_in">input</span>[<span class="number">2</span>] * <span class="variable language_">self</span>.input3_w2) + </span><br><span class="line">                               (<span class="built_in">input</span>[<span class="number">3</span>] * <span class="variable language_">self</span>.input4_w2))</span><br><span class="line">        </span><br><span class="line">        output1 = (input_to_top_hidden * <span class="variable language_">self</span>.output1_w1 + </span><br><span class="line">                   input_to_bottom_hidden * <span class="variable language_">self</span>.output1_w2)</span><br><span class="line">        output2 = (input_to_top_hidden * <span class="variable language_">self</span>.output2_w1 + </span><br><span class="line">                   input_to_bottom_hidden * <span class="variable language_">self</span>.output2_w2)</span><br><span class="line">        output3 = (input_to_top_hidden * <span class="variable language_">self</span>.output3_w1 + </span><br><span class="line">                   input_to_bottom_hidden * <span class="variable language_">self</span>.output3_w2)</span><br><span class="line">        output4 = (input_to_top_hidden * <span class="variable language_">self</span>.output4_w1 + </span><br><span class="line">                   input_to_bottom_hidden * <span class="variable language_">self</span>.output4_w2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if not torch.stackï¼Œjust have the [], the graph of gradient will disappeared</span></span><br><span class="line">        output_presoftmax = torch.stack([output1, output2, output3, output4])</span><br><span class="line">        <span class="keyword">return</span> output_presoftmax</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> Adam(<span class="variable language_">self</span>.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        intput_i, label_i = batch</span><br><span class="line">        output_i = <span class="variable language_">self</span>.forward(intput_i)</span><br><span class="line">        loss  = <span class="variable language_">self</span>.loss(output_i, label_i[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    </span><br><span class="line">modelFromScratch = WordEmbeddingFromScratch()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Before optimization, the parameters are ...&quot;</span>)</span><br><span class="line"><span class="comment"># print the initialize all the named parameters</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> modelFromScratch.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>è¾“å‡ºç»“æœä¸ºï¼š</p>
<p>input1_w1 tensor(0.4502) ...</p>
<p>output4_w1 tensor(-0.4579)</p>
<p>output4_w2 tensor(0.3754)</p>
</blockquote>
<p>ä¸ºäº†æ›´å¥½çš„æŸ¥çœ‹w1å’Œw2è¿›è¡Œä¸‹é¢çš„æ“ä½œï¼Œã€è¡¨æ ¼å±•ç¤ºå’Œå›¾è¡¨å±•ç¤ºã€‘</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;w1&quot;</span>: [modelFromScratch.input1_w1.item(),</span><br><span class="line">           modelFromScratch.input2_w1.item(),</span><br><span class="line">           modelFromScratch.input3_w1.item(),</span><br><span class="line">           modelFromScratch.input4_w1.item()],</span><br><span class="line">    <span class="string">&quot;w2&quot;</span>: [modelFromScratch.input1_w2.item(),</span><br><span class="line">           modelFromScratch.input2_w2.item(),</span><br><span class="line">           modelFromScratch.input3_w2.item(),</span><br><span class="line">           modelFromScratch.input4_w2.item()],</span><br><span class="line">    <span class="string">&quot;token&quot;</span>: [<span class="string">&quot;Troll2&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;great&quot;</span>, <span class="string">&quot;Gym&quot;</span>],</span><br><span class="line">    <span class="string">&quot;input&quot;</span>: [<span class="string">&quot;input1&quot;</span>,<span class="string">&quot;input2&quot;</span>,<span class="string">&quot;input3&quot;</span>,<span class="string">&quot;input4&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data=data)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line">sns.scatterplot(df, x =<span class="string">&quot;w1&quot;</span>, y= <span class="string">&quot;w2&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(df.w1[<span class="number">0</span>], df.w2[<span class="number">0</span>],df.token[<span class="number">0</span>],</span><br><span class="line">         horizontalalignment = <span class="string">&quot;left&quot;</span>,</span><br><span class="line">         size = <span class="string">&quot;medium&quot;</span>, </span><br><span class="line">         color = <span class="string">&quot;black&quot;</span>,</span><br><span class="line">         weight = <span class="string">&quot;semibold&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(df.w1[<span class="number">1</span>], df.w2[<span class="number">1</span>],df.token[<span class="number">1</span>],</span><br><span class="line">         horizontalalignment = <span class="string">&quot;left&quot;</span>,</span><br><span class="line">         size = <span class="string">&quot;medium&quot;</span>, </span><br><span class="line">         color = <span class="string">&quot;black&quot;</span>,</span><br><span class="line">         weight = <span class="string">&quot;semibold&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(df.w1[<span class="number">2</span>], df.w2[<span class="number">2</span>],df.token[<span class="number">2</span>],</span><br><span class="line">         horizontalalignment = <span class="string">&quot;left&quot;</span>,</span><br><span class="line">         size = <span class="string">&quot;medium&quot;</span>, </span><br><span class="line">         color = <span class="string">&quot;black&quot;</span>,</span><br><span class="line">         weight = <span class="string">&quot;semibold&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.text(df.w1[<span class="number">3</span>], df.w2[<span class="number">3</span>],df.token[<span class="number">3</span>],</span><br><span class="line">         horizontalalignment = <span class="string">&quot;left&quot;</span>,</span><br><span class="line">         size = <span class="string">&quot;medium&quot;</span>, </span><br><span class="line">         color = <span class="string">&quot;black&quot;</span>,</span><br><span class="line">         weight = <span class="string">&quot;semibold&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a trainer</span></span><br><span class="line">trainer = L.Trainer(max_epochs=<span class="number">100</span>) <span class="comment"># max epochs = 100</span></span><br><span class="line">trainer.fit(modelFromScratch, train_dataloaders=dataLoader)   </span><br></pre></td></tr></table></figure>
<p>å¯è§†åŒ–ç»“æœå¦‚ä¸‹ï¼Œå½“ç„¶æ¯æ¬¡çš„ç»“æœéƒ½åº”è¯¥æ˜¯ä¸ä¸€æ ·çš„ï¼š</p>
<p><a href="image-20241006214611884.png" title="image-20241006214611884" class="gallery-item" style="box-shadow: none;"> <img src="image-20241006214611884.png" alt="image-20241006214611884" style="zoom:50%;" /></a></p>
<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 13%" />
<col style="width: 11%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">w1</th>
<th style="text-align: center;">w2</th>
<th style="text-align: center;">token</th>
<th style="text-align: center;">input</th>
<th style="text-align: center;">w1'</th>
<th style="text-align: center;">w2'</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.115312</td>
<td style="text-align: center;">-0.237109</td>
<td style="text-align: center;">Troll2</td>
<td style="text-align: center;">input1</td>
<td style="text-align: center;">-1.538855</td>
<td style="text-align: center;">-1.861845</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.127418</td>
<td style="text-align: center;">0.372505</td>
<td style="text-align: center;">is</td>
<td style="text-align: center;">input2</td>
<td style="text-align: center;">-2.131814</td>
<td style="text-align: center;">1.852834</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.253114</td>
<td style="text-align: center;">0.034632</td>
<td style="text-align: center;">a</td>
<td style="text-align: center;">input3</td>
<td style="text-align: center;">2.612083</td>
<td style="text-align: center;">0.616395</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">-0.067604</td>
<td style="text-align: center;">-0.073453</td>
<td style="text-align: center;">Gymkata</td>
<td style="text-align: center;">input4</td>
<td style="text-align: center;">-1.189931</td>
<td style="text-align: center;">-2.070752</td>
</tr>
</tbody>
</table>
<p>æ¥ä¸‹æ¥é€šè¿‡nn.Linear()å‡½æ•°å¯¹modelè¿›è¡Œç®€åŒ–ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WordEmbeddingWithLinear</span>(L.LightningModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_to_hidden = nn.Linear(in_features=<span class="number">4</span>, out_features=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.hidden_to_output = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">4</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        hidden = <span class="variable language_">self</span>.input_to_hidden(<span class="built_in">input</span>)</span><br><span class="line">        output_values = <span class="variable language_">self</span>.hidden_to_output(hidden)</span><br><span class="line">        <span class="keyword">return</span> output_values</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> Adam(<span class="variable language_">self</span>.parameters(), lr = <span class="number">0.1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">        input_i, label_1 = batch</span><br><span class="line">        output_i = <span class="variable language_">self</span>.forward(input_i)</span><br><span class="line">        loss = <span class="variable language_">self</span>.loss(output_i, label_1)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">modelLinear = WordEmbeddingWithLinear()</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;w1&quot;</span>: modelLinear.input_to_hidden.weight.detach()[<span class="number">0</span>].numpy(),</span><br><span class="line">    <span class="string">&quot;w2&quot;</span>: modelLinear.input_to_hidden.weight.detach()[<span class="number">1</span>].numpy(),</span><br><span class="line">    <span class="string">&quot;token&quot;</span>: [<span class="string">&quot;Troll2&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;great&quot;</span>, <span class="string">&quot;Gym&quot;</span>],</span><br><span class="line">    <span class="string">&quot;input&quot;</span>: [<span class="string">&quot;input1&quot;</span>,<span class="string">&quot;input2&quot;</span>,<span class="string">&quot;input3&quot;</span>,<span class="string">&quot;input4&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>modelLinear.input_to_hidden.weightæ˜¯ä¸€ä¸ª2*4çš„tensorï¼Œä¹Ÿå°±æ˜¯ç¬¬ä¸€è¡Œæ˜¯w1æƒé‡ã€‚<strong><span style="color: red">ä»è¿™ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œnn.Linearä¹‹åäº§ç”Ÿçš„å¯¹è±¡çš„weightä¸­ï¼Œè¡Œå‘é‡å¯¹åº”çš„è¾“å‡ºçš„ä¸€ä¸ªhiddenç¥ç»å…ƒ</span></strong>ã€‚åˆ—å°±æ˜¯æ¯ä¸ªè¾“å…¥å•è¯å¯¹æ‰€æœ‰çš„hiddençš„æƒé‡</p></li>
<li><p>.detach()å‡½æ•°æ˜¯å»é™¤æ¢¯åº¦å‡½æ•°</p></li>
</ul>
<p>åœ¨ä¹‹åçš„è¯å°±æ˜¯éœ€è¦ä½¿ç”¨nn.Embeddingå¯¹è®­ç»ƒå‡ºæ¥çš„å‚æ•°è¿›è¡Œä½¿ç”¨äº†</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_embeddings = nn.Embedding.from_pretrained(modelLinear.input_to_hidden.weight.T)</span><br><span class="line">word_embeddings.weight <span class="comment"># 4 * 2</span></span><br></pre></td></tr></table></figure>
</div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2024-10-06</span>
            
                <span>è¯¥ç¯‡æ–‡ç« è¢« Xue xt</span>
            
            
             
                <span>å½’ä¸ºåˆ†ç±»:
                    
                    
                        <a href='/categories/Transform/'>
                            Transform
                        </a>
                    
                </span>
            
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>ä¸Šä¸€ç¯‡ï¼š<a href='/2024/10/07/Intepolation/'>Games001 | InterpolationAndFittings</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">ä¸‹ä¸€ç¯‡ï¼š<a href="/2024/10/06/ReinforcementLearning/">ReinforcementLearning</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            Â© 2001-2024 Xuext 

            
                

            
        </span>
       
    
</div>



<!--è¿™æ˜¯æŒ‡ä¸€æ¡çº¿å¾€ä¸‹çš„å†…å®¹-->
<div class="footer-last">
    
            <span>ğŸŒŠçœ‹è¿‡å¤§æµ·çš„äººä¸ä¼šå¿˜è®°æµ·çš„å¹¿é˜”ğŸŒŠ</span>
            
                <span class="footer-last-span-right"><i>æœ¬ç«™ç”±<b style="color: #333333;">ä½™é‡‘æ‚¦å¥³å£«</b>äº²æƒ…æ”¯æŒ ğŸ‘ </i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--ç›®å½•-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--å›åˆ°é¡¶éƒ¨æŒ‰é’®-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // å¯ç”¨æ’ä»¶
            thumbnail: true,          // æ˜¾ç¤ºç¼©ç•¥å›¾
            zoom: true,               // å¯ç”¨ç¼©æ”¾åŠŸ
            rotate: true,             // å¯ç”¨æ—‹è½¬åŠŸèƒ½èƒ½
            autoplay: true,        // å¯ç”¨è‡ªåŠ¨æ’­æ”¾åŠŸèƒ½
            fullScreen: true,      // å¯ç”¨å…¨å±åŠŸèƒ½
            pager: false, //é¡µç ,
            zoomFromOrigin: true,   // ä»åŸå§‹ä½ç½®ç¼©æ”¾
            actualSize: true,       // å¯ç”¨æŸ¥çœ‹å®é™…å¤§å°çš„åŠŸèƒ½
            enableZoomAfter: 300,    // å»¶è¿Ÿç¼©æ”¾ï¼Œç¡®ä¿å›¾ç‰‡åŠ è½½å®Œæˆåå¯ç¼©æ”¾
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // ä¿®å¤é€‰æ‹©å™¨
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- å›åˆ°é¡¶éƒ¨çš„æŒ‰é’®-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- è¿”å›çš„æŒ‰é’®-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>